{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":10090,"databundleVersionId":111024},{"sourceType":"competition","sourceId":3136,"databundleVersionId":26502},{"sourceType":"datasetVersion","sourceId":1777920,"datasetId":1057064,"databundleVersionId":1815287},{"sourceType":"datasetVersion","sourceId":3011626,"datasetId":1844693,"databundleVersionId":3059519},{"sourceType":"datasetVersion","sourceId":14980873,"datasetId":9589506,"databundleVersionId":15854096},{"sourceType":"datasetVersion","sourceId":23498,"datasetId":310,"databundleVersionId":23502},{"sourceType":"datasetVersion","sourceId":4289678,"datasetId":2527538,"databundleVersionId":4347518},{"sourceType":"datasetVersion","sourceId":477177,"datasetId":216167,"databundleVersionId":493143},{"sourceType":"datasetVersion","sourceId":1898721,"datasetId":1131493,"databundleVersionId":1937036},{"sourceType":"datasetVersion","sourceId":34877,"datasetId":27352,"databundleVersionId":35935},{"sourceType":"datasetVersion","sourceId":77759,"datasetId":339,"databundleVersionId":80217},{"sourceType":"datasetVersion","sourceId":14981397,"datasetId":9589863,"databundleVersionId":15854655},{"sourceType":"datasetVersion","sourceId":6347913,"datasetId":3655505,"databundleVersionId":6428440}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Task 1: Handling Missing Data – Titanic Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task identifies and resolves missing values in the Titanic dataset to prepare it for machine learning.\n\n**Techniques:** Median imputation for numerical data, Mode imputation for categorical data, and Column dropping for features with excessive missingness.\n\n**Reason:** Median is used for 'Age' to avoid outlier influence; Mode is used for 'Embarked' as it is categorical; 'Cabin' is dropped because over 70% of its data is missing.","metadata":{}},{"cell_type":"code","source":"import pandas as pd ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:22.691797Z","iopub.execute_input":"2026-02-27T17:50:22.692181Z","iopub.status.idle":"2026-02-27T17:50:22.696458Z","shell.execute_reply.started":"2026-02-27T17:50:22.692152Z","shell.execute_reply":"2026-02-27T17:50:22.695606Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/competitions/titanic/train.csv')\n\n# Missing Values BEFORE Handling\nprint(\"Missing Values Before Preprocessing:\\n\")\nmissing_before = df.isnull().sum()\nprint(missing_before)\n\n# 1. Median Imputation for 'Age'\ndf['Age'] = df['Age'].fillna(df['Age'].median())\n\n# 2. Mode Imputation for 'Embarked' \ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n\n# 3. Dropping 'Cabin' column (too many missing values)\ndf.drop(columns=['Cabin'], inplace=True)\n\n# 4. Dropping rows with any remaining nulls (if any)\ndf.dropna(inplace=True)\n\n# Missing Values AFTER Handling\nprint(\"\\nMissing Values After Preprocessing:\\n\")\nmissing_after = df.isnull().sum()\nprint(missing_after)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:22.714839Z","iopub.execute_input":"2026-02-27T17:50:22.715562Z","iopub.status.idle":"2026-02-27T17:50:22.744267Z","shell.execute_reply.started":"2026-02-27T17:50:22.715523Z","shell.execute_reply":"2026-02-27T17:50:22.743223Z"}},"outputs":[{"name":"stdout","text":"Missing Values Before Preprocessing:\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\nMissing Values After Preprocessing:\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nEmbarked       0\ndtype: int64\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# *Task 2: Feature Encoding – Car Evaluation Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task converts categorical text data into numerical format using two different encoding techniques.\n\n**Techniques:** One-Hot Encoding and Label Encoding.\n\n**Reason:** Label Encoding is used for the 'target_class' to maintain a single column, while One-Hot Encoding is used for features to avoid implying an incorrect mathematical order between categories like 'vhigh' and 'med'.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# 1. Load data and manually assign the correct names\n# 'unacc' is the target (Class), others are features\ncol_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'target_class']\npath = '/kaggle/input/datasets/ayeshaaniqa/car-evaluation/car.data' \ndf_car = pd.read_csv(path, names=col_names)\n\n# 2. Label Encoding (Applied to the 'target_class' column)\nle = LabelEncoder()\ndf_car['target_class_encoded'] = le.fit_transform(df_car['target_class'])\n\n# 3. One-Hot Encoding (Applied to all feature columns)\ndf_encoded = pd.get_dummies(df_car, columns=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n\n# Displaying result\ndf_encoded.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:22.745855Z","iopub.execute_input":"2026-02-27T17:50:22.746261Z","iopub.status.idle":"2026-02-27T17:50:22.786689Z","shell.execute_reply.started":"2026-02-27T17:50:22.746234Z","shell.execute_reply":"2026-02-27T17:50:22.785910Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"  target_class  target_class_encoded  buying_high  buying_low  buying_med  \\\n0        unacc                     2        False       False       False   \n1        unacc                     2        False       False       False   \n2        unacc                     2        False       False       False   \n3        unacc                     2        False       False       False   \n4        unacc                     2        False       False       False   \n\n   buying_vhigh  maint_high  maint_low  maint_med  maint_vhigh  ...  \\\n0          True       False      False      False         True  ...   \n1          True       False      False      False         True  ...   \n2          True       False      False      False         True  ...   \n3          True       False      False      False         True  ...   \n4          True       False      False      False         True  ...   \n\n   doors_5more  persons_2  persons_4  persons_more  lug_boot_big  \\\n0        False       True      False         False         False   \n1        False       True      False         False         False   \n2        False       True      False         False         False   \n3        False       True      False         False         False   \n4        False       True      False         False         False   \n\n   lug_boot_med  lug_boot_small  safety_high  safety_low  safety_med  \n0         False            True        False        True       False  \n1         False            True        False       False        True  \n2         False            True         True       False       False  \n3          True           False        False        True       False  \n4          True           False        False       False        True  \n\n[5 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target_class</th>\n      <th>target_class_encoded</th>\n      <th>buying_high</th>\n      <th>buying_low</th>\n      <th>buying_med</th>\n      <th>buying_vhigh</th>\n      <th>maint_high</th>\n      <th>maint_low</th>\n      <th>maint_med</th>\n      <th>maint_vhigh</th>\n      <th>...</th>\n      <th>doors_5more</th>\n      <th>persons_2</th>\n      <th>persons_4</th>\n      <th>persons_more</th>\n      <th>lug_boot_big</th>\n      <th>lug_boot_med</th>\n      <th>lug_boot_small</th>\n      <th>safety_high</th>\n      <th>safety_low</th>\n      <th>safety_med</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>unacc</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>unacc</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>unacc</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>unacc</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>unacc</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>...</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# *Task 3: Feature Scaling – Wine Quality Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves adjusting the scale of numerical features in the Wine Quality dataset to ensure that no single feature dominates the model due to its magnitude.\n\n**Technique:** Normalization (Min-Max Scaling) and Standardization (Z-score Scaling).\n\n**Reason:** Normalization is used to bound features between 0 and 1, which is useful for algorithms like KNN. Standardization is used to center the data around a mean of 0 with a standard deviation of 1, which is preferred for linear models and PCA.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\n\npath = '/kaggle/input/datasets/ayeshaaniqa/red-wine-quality/winequality-red.csv' \ndf_wine = pd.read_csv(path, sep=';')\n\n# Select numerical features for scaling (exclude target 'quality' if present)\nfeatures = df_wine.drop(columns=['quality'])\n\n# 1. Normalization (Min-Max Scaling)\nscaler_minmax = MinMaxScaler()\ndf_normalized = pd.DataFrame(scaler_minmax.fit_transform(features), columns=features.columns)\nprint(\"Normalized: \", df_normalized.head())\n\n# 2. Standardization (Standard Scaling)\nscaler_standard = StandardScaler()\ndf_standardized = pd.DataFrame(scaler_standard.fit_transform(features), columns=features.columns)\nprint(\"Standardized: \", df_standardized.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:22.787882Z","iopub.execute_input":"2026-02-27T17:50:22.788291Z","iopub.status.idle":"2026-02-27T17:50:22.821290Z","shell.execute_reply.started":"2026-02-27T17:50:22.788261Z","shell.execute_reply":"2026-02-27T17:50:22.820399Z"}},"outputs":[{"name":"stdout","text":"Normalized:     fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0       0.247788          0.397260         0.00        0.068493   0.106845   \n1       0.283186          0.520548         0.00        0.116438   0.143573   \n2       0.283186          0.438356         0.04        0.095890   0.133556   \n3       0.584071          0.109589         0.56        0.068493   0.105175   \n4       0.247788          0.397260         0.00        0.068493   0.106845   \n\n   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n0             0.140845              0.098940  0.567548  0.606299   0.137725   \n1             0.338028              0.215548  0.494126  0.362205   0.209581   \n2             0.197183              0.169611  0.508811  0.409449   0.191617   \n3             0.225352              0.190813  0.582232  0.330709   0.149701   \n4             0.140845              0.098940  0.567548  0.606299   0.137725   \n\n    alcohol  \n0  0.153846  \n1  0.215385  \n2  0.215385  \n3  0.215385  \n4  0.153846  \nStandardized:     fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0      -0.528360          0.961877    -1.391472       -0.453218  -0.243707   \n1      -0.298547          1.967442    -1.391472        0.043416   0.223875   \n2      -0.298547          1.297065    -1.186070       -0.169427   0.096353   \n3       1.654856         -1.384443     1.484154       -0.453218  -0.264960   \n4      -0.528360          0.961877    -1.391472       -0.453218  -0.243707   \n\n   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n0            -0.466193             -0.379133  0.558274  1.288643  -0.579207   \n1             0.872638              0.624363  0.028261 -0.719933   0.128950   \n2            -0.083669              0.229047  0.134264 -0.331177  -0.048089   \n3             0.107592              0.411500  0.664277 -0.979104  -0.461180   \n4            -0.466193             -0.379133  0.558274  1.288643  -0.579207   \n\n    alcohol  \n0 -0.960246  \n1 -0.584777  \n2 -0.584777  \n3 -0.584777  \n4 -0.960246  \n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# *Task 4: Handling Outliers – Boston Housing Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves identifying and treating extreme values (outliers) in the Boston Housing dataset to prevent them from negatively impacting the performance of regression models.\n\n**Technique:** Z-score method to identify outliers and Winsorization (capping) to treat them.\n\n**Reason:** The Z-score method is suitable for identifying data points that are significantly far from the mean. Winsorization is chosen over deletion to preserve data points while reducing the influence of extreme values on the model.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nimport numpy as np\n# Data loading\npath = '/kaggle/input/competitions/boston-dataset/boston_data.csv' \ndf_boston = pd.read_csv(path)\n\n# Select numerical features for outlier detection\nnumerical_cols = df_boston.select_dtypes(include=[np.number]).columns\n\n# 1. Identify Outliers using Z-score (threshold > 3)\nz_scores = np.abs(stats.zscore(df_boston[numerical_cols]))\noutlier_mask = (z_scores > 3).any(axis=1)\n\n# 2. Treat Outliers using Winsorization\ndf_treated = df_boston.copy()\nfor col in numerical_cols:\n    lower_limit = df_treated[col].quantile(0.05)\n    upper_limit = df_treated[col].quantile(0.95)\n    df_treated[col] = np.where(df_treated[col] < lower_limit, lower_limit,\n                               np.where(df_treated[col] > upper_limit, upper_limit, df_treated[col]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:22.822919Z","iopub.execute_input":"2026-02-27T17:50:22.823842Z","iopub.status.idle":"2026-02-27T17:50:22.867446Z","shell.execute_reply.started":"2026-02-27T17:50:22.823811Z","shell.execute_reply":"2026-02-27T17:50:22.866339Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(\"\\nDescription of features before treatment:\")\nprint(df_boston[numerical_cols].describe())\nprint(\"\\nDescription of features after Winsorization:\")\nprint(df_treated[numerical_cols].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:22.869105Z","iopub.execute_input":"2026-02-27T17:50:22.870123Z","iopub.status.idle":"2026-02-27T17:50:22.931847Z","shell.execute_reply.started":"2026-02-27T17:50:22.870074Z","shell.execute_reply":"2026-02-27T17:50:22.930918Z"}},"outputs":[{"name":"stdout","text":"\nDescription of features before treatment:\n             crim          zn       indus        chas         nox         rm  \\\ncount  404.000000  404.000000  404.000000  404.000000  404.000000  404.00000   \nmean     3.730912   10.509901   11.189901    0.069307    0.556710    6.30145   \nstd      8.943922   22.053733    6.814909    0.254290    0.117321    0.67583   \nmin      0.006320    0.000000    0.460000    0.000000    0.392000    3.56100   \n25%      0.082382    0.000000    5.190000    0.000000    0.453000    5.90275   \n50%      0.253715    0.000000    9.795000    0.000000    0.538000    6.23050   \n75%      4.053158   12.500000   18.100000    0.000000    0.631000    6.62925   \nmax     88.976200   95.000000   27.740000    1.000000    0.871000    8.78000   \n\n              age         dis         rad         tax     ptratio       black  \\\ncount  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \nmean    68.601733    3.799666    9.836634  411.688119   18.444554  355.068243   \nstd     28.066143    2.109916    8.834741  171.073553    2.150295   94.489572   \nmin      2.900000    1.169100    1.000000  187.000000   12.600000    0.320000   \n25%     45.800000    2.087875    4.000000  281.000000   17.375000  374.710000   \n50%     76.600000    3.207450    5.000000  330.000000   19.000000  391.065000   \n75%     94.150000    5.222125   24.000000  666.000000   20.200000  396.007500   \nmax    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n\n            lstat        medv  \ncount  404.000000  404.000000  \nmean    12.598936   22.312376  \nstd      6.925173    8.837019  \nmin      1.730000    5.000000  \n25%      7.135000   17.100000  \n50%     11.265000   21.400000  \n75%     16.910000   25.000000  \nmax     34.370000   50.000000  \n\nDescription of features after Winsorization:\n             crim          zn       indus        chas         nox          rm  \\\ncount  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \nmean     2.861964   10.076733   11.005204    0.069307    0.553299    6.299589   \nstd      4.660114   20.607770    6.407387    0.254290    0.108188    0.541874   \nmin      0.028786    0.000000    2.052500    0.000000    0.410150    5.391350   \n25%      0.082382    0.000000    5.190000    0.000000    0.453000    5.902750   \n50%      0.253715    0.000000    9.795000    0.000000    0.538000    6.230500   \n75%      4.053158   12.500000   18.100000    0.000000    0.631000    6.629250   \nmax     15.532545   75.000000   19.580000    1.000000    0.765500    7.462500   \n\n              age         dis         rad         tax     ptratio       black  \\\ncount  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \nmean    68.968564    3.741042    9.868812  412.512376   18.492822  357.517740   \nstd     27.321399    1.937387    8.804190  168.809456    2.010808   85.960452   \nmin     17.500000    1.470560    2.000000  222.000000   14.700000   70.807000   \n25%     45.800000    2.087875    4.000000  281.000000   17.375000  374.710000   \n50%     76.600000    3.207450    5.000000  330.000000   19.000000  391.065000   \n75%     94.150000    5.222125   24.000000  666.000000   20.200000  396.007500   \nmax    100.000000    7.827605   24.000000  666.000000   21.000000  396.900000   \n\n            lstat        medv  \ncount  404.000000  404.000000  \nmean    12.491494   22.105136  \nstd      6.511319    7.735101  \nmin      3.962000   10.200000  \n25%      7.135000   17.100000  \n50%     11.265000   21.400000  \n75%     16.910000   25.000000  \nmax     26.611500   41.075000  \n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# *Task 5: Data Imputation (Advanced ) – Retail Sales Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves filling in missing numerical values in the Retail Sales dataset using advanced statistical imputation techniques rather than simple mean/median methods.\n\n**Technique:** K-Nearest Neighbors (KNN) Imputation and MICE (Multivariate Imputation by Chained Equations).\n\n**Reason:** Advanced techniques like KNN and MICE model relationships between features to make more accurate imputations, reducing bias compared to simple imputation","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Data loading - replace with your copied path from the sidebar\npath = '/kaggle/input/datasets/mohammadtalib786/retail-sales-dataset/retail_sales_dataset.csv' \ndf_retail = pd.read_csv(path)\n\n# Display missing values before imputation\nprint(\"Missing values before imputation:\")\nprint(df_retail.isnull().sum())\n\n# Select numerical columns for imputation\nnumerical_cols = df_retail.select_dtypes(include=['float64', 'int64']).columns\ndf_numeric = df_retail[numerical_cols].copy()\n\n# 1. KNN Imputation\nknn_imputer = KNNImputer(n_neighbors=5)\ndf_knn = pd.DataFrame(knn_imputer.fit_transform(df_numeric), columns=numerical_cols)\n\n# 2. MICE Imputation\nmice_imputer = IterativeImputer(random_state=0)\ndf_mice = pd.DataFrame(mice_imputer.fit_transform(df_numeric), columns=numerical_cols)\n\n# Display missing values after imputation to confirm\nprint(\"\\nMissing values after KNN imputation:\")\nprint(df_knn.isnull().sum())\nprint(\"\\nMissing values after MICE imputation:\")\nprint(df_mice.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:22.933101Z","iopub.execute_input":"2026-02-27T17:50:22.933809Z","iopub.status.idle":"2026-02-27T17:50:23.191116Z","shell.execute_reply.started":"2026-02-27T17:50:22.933779Z","shell.execute_reply":"2026-02-27T17:50:23.190280Z"}},"outputs":[{"name":"stdout","text":"Missing values before imputation:\nTransaction ID      0\nDate                0\nCustomer ID         0\nGender              0\nAge                 0\nProduct Category    0\nQuantity            0\nPrice per Unit      0\nTotal Amount        0\ndtype: int64\n\nMissing values after KNN imputation:\nTransaction ID    0\nAge               0\nQuantity          0\nPrice per Unit    0\nTotal Amount      0\ndtype: int64\n\nMissing values after MICE imputation:\nTransaction ID    0\nAge               0\nQuantity          0\nPrice per Unit    0\nTotal Amount      0\ndtype: int64\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Task 6: Feature Engineering – Heart Disease Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves creating new, meaningful features from existing data to help a machine learning model capture patterns more effectively.\n\n**Technique:** Creating derived categorical features: \"Age Group\" and \"Cholesterol Level\".\n\n**Reason:** Raw continuous values like age and cholesterol can be grouped into clinically relevant categories, which helps simplify complex relationships for certain types of models","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/datasets/johnsmith88/heart-disease-dataset/heart.csv' \ndf_heart = pd.read_csv(path)\n\n# Display original data sample\nprint(\"Original features (Age and Chol):\")\nprint(df_heart[['age', 'chol']].head())\n\n# 1. Derived Feature: Age Groups\ndef get_age_group(age):\n    if age < 40: return 'Young'\n    elif age <= 60: return 'Middle-Aged'\n    else: return 'Senior'\n\ndf_heart['age_group'] = df_heart['age'].apply(get_age_group)\n\n# 2. Derived Feature: Cholesterol Categories\ndef get_chol_category(chol):\n    if chol < 200: return 'Normal'\n    elif chol < 240: return 'Borderline'\n    else: return 'High'\n\ndf_heart['chol_category'] = df_heart['chol'].apply(get_chol_category)\n\n# Display Results\nprint(\"\\nNew Derived Features:\")\nprint(df_heart[['age', 'age_group', 'chol', 'chol_category']].head())\n\nprint(\"\\nDistribution of New Features:\")\nprint(df_heart['age_group'].value_counts())\nprint(df_heart['chol_category'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:23.192141Z","iopub.execute_input":"2026-02-27T17:50:23.192772Z","iopub.status.idle":"2026-02-27T17:50:23.226904Z","shell.execute_reply.started":"2026-02-27T17:50:23.192745Z","shell.execute_reply":"2026-02-27T17:50:23.226049Z"}},"outputs":[{"name":"stdout","text":"Original features (Age and Chol):\n   age  chol\n0   52   212\n1   53   203\n2   70   174\n3   61   203\n4   62   294\n\nNew Derived Features:\n   age    age_group  chol chol_category\n0   52  Middle-Aged   212    Borderline\n1   53  Middle-Aged   203    Borderline\n2   70       Senior   174        Normal\n3   61       Senior   203    Borderline\n4   62       Senior   294          High\n\nDistribution of New Features:\nage_group\nMiddle-Aged    696\nSenior         272\nYoung           57\nName: count, dtype: int64\nchol_category\nHigh          517\nBorderline    339\nNormal        169\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# *Task 7: Variable Transformation – Bike Sharing Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves applying mathematical transformations to numerical features in the Bike Sharing dataset to handle skewed data and stabilize variance.\n\n**Technique:** Log Transformation and Box-Cox Transformation.\n\n**Reason:** Many machine learning algorithms assume that features follow a normal (Gaussian) distribution. Log and Box-Cox transformations help \"un-skew\" data that is clustered on one side, making the relationship between variables more linear and improving model performance.","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/datasets/yasserh/bike-sharing-dataset/day.csv' \ndf_bike = pd.read_csv(path)\n\n# 1. Original Skewness\noriginal_skew = df_bike['cnt'].skew()\nprint(f\"Original Skewness of 'cnt': {original_skew:.4f}\")\n\n# 2. Log Transformation (using log1p to handle 0s)\ndf_bike['cnt_log'] = np.log1p(df_bike['cnt'])\n\n# 3. Box-Cox Transformation (requires positive values)\ndf_bike['cnt_boxcox'], _ = stats.boxcox(df_bike['cnt'])\n\n# 4. Square Root Transformation\ndf_bike['cnt_sqrt'] = np.sqrt(df_bike['cnt'])\n\n# Display Results\nprint(\"\\n--- Skewness Comparison ---\")\nprint(f\"Log Transform: {df_bike['cnt_log'].skew():.4f}\")\nprint(f\"Box-Cox Transform: {df_bike['cnt_boxcox'].skew():.4f}\")\nprint(f\"Square Root Transform: {df_bike['cnt_sqrt'].skew():.4f}\")\n\nprint(\"\\n--- Transformed Data (First 5 Rows) ---\")\nprint(df_bike[['cnt', 'cnt_log', 'cnt_boxcox', 'cnt_sqrt']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:23.227891Z","iopub.execute_input":"2026-02-27T17:50:23.228182Z","iopub.status.idle":"2026-02-27T17:50:23.278919Z","shell.execute_reply.started":"2026-02-27T17:50:23.228158Z","shell.execute_reply":"2026-02-27T17:50:23.278048Z"}},"outputs":[{"name":"stdout","text":"Original Skewness of 'cnt': -0.0496\n\n--- Skewness Comparison ---\nLog Transform: -1.9154\nBox-Cox Transform: -0.1544\nSquare Root Transform: -0.5794\n\n--- Transformed Data (First 5 Rows) ---\n    cnt   cnt_log  cnt_boxcox   cnt_sqrt\n0   985  6.893656  505.996549  31.384710\n1   801  6.687109  421.088686  28.301943\n2  1349  7.207860  668.973663  36.728735\n3  1562  7.354362  761.935717  39.522146\n4  1600  7.378384  778.363243  40.000000\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# *Task 8: Feature Selection – Diabetes Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task focuses on identifying the most relevant features in the Pima Indians Diabetes dataset to reduce model complexity and improve performance.\n\n**Technique:** Correlation Analysis and Chi-Square Test.\n\n**Reason:** Correlation Analysis helps identify linear relationships between numerical features, while the Chi-Square test evaluates the independence of categorical features relative to the target, allowing us to drop redundant or irrelevant data.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectKBest, chi2\n\npath = '/kaggle/input/datasets/akshaydattatraykhare/diabetes-dataset/diabetes.csv' \ndf_diabetes = pd.read_csv(path)\n\n# 1. Correlation Analysis\ncorrelation_matrix = df_diabetes.corr()\nprint(\"Correlation with Target (Outcome):\")\nprint(correlation_matrix['Outcome'].sort_values(ascending=False))\n\n# 2. Chi-Square Test for Feature Selection\nX = df_diabetes.drop(columns=['Outcome'])\ny = df_diabetes['Outcome']\n\n# Apply SelectKBest to extract top 4 best features\nbestfeatures = SelectKBest(score_func=chi2, k=4)\nfit = bestfeatures.fit(X, y)\n\n# Create a dataframe to view scores\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns, dfscores], axis=1)\nfeatureScores.columns = ['Feature', 'Score']\n\nprint(\"\\n--- Top 4 Features via Chi-Square Test ---\")\nprint(featureScores.nlargest(4, 'Score'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:23.281417Z","iopub.execute_input":"2026-02-27T17:50:23.281679Z","iopub.status.idle":"2026-02-27T17:50:23.832993Z","shell.execute_reply.started":"2026-02-27T17:50:23.281654Z","shell.execute_reply":"2026-02-27T17:50:23.832172Z"}},"outputs":[{"name":"stdout","text":"Correlation with Target (Outcome):\nOutcome                     1.000000\nGlucose                     0.466581\nBMI                         0.292695\nAge                         0.238356\nPregnancies                 0.221898\nDiabetesPedigreeFunction    0.173844\nInsulin                     0.130548\nSkinThickness               0.074752\nBloodPressure               0.065068\nName: Outcome, dtype: float64\n\n--- Top 4 Features via Chi-Square Test ---\n   Feature        Score\n4  Insulin  2175.565273\n1  Glucose  1411.887041\n7      Age   181.303689\n5      BMI   127.669343\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# *Task 9: Handling/Dealing w Imbalanced Data – Credit Card Fraud Detection*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task addresses the extreme class imbalance in fraud detection datasets, where legitimate transactions far outnumber fraudulent ones.\n\n**Technique:** Random Undersampling and SMOTE (Synthetic Minority Over-sampling Technique).\n\n**Reason:** Models trained on imbalanced data often ignore the minority class (fraud). Undersampling balances the classes by removing majority samples, while SMOTE creates synthetic fraudulent samples to provide the model with more patterns to learn from","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.utils import resample\n\npath = '/kaggle/input/datasets/organizations/mlg-ulb/creditcardfraud/creditcard.csv' \ndf_fraud = pd.read_csv(path)\ndf_fraud.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:23.834012Z","iopub.execute_input":"2026-02-27T17:50:23.834357Z","iopub.status.idle":"2026-02-27T17:50:28.383191Z","shell.execute_reply.started":"2026-02-27T17:50:23.834329Z","shell.execute_reply":"2026-02-27T17:50:28.382130Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"\n# Display original class distribution\nprint(\"Original Class Distribution:\")\nprint(df_fraud['Class'].value_counts())\n\n# Separate majority and minority classes\ndf_majority = df_fraud[df_fraud.Class == 0]\ndf_minority = df_fraud[df_fraud.Class == 1]\n\n# Downsample majority class to match minority class size\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    \n                                 n_samples=len(df_minority),\n                                 random_state=42)\n\ndf_undersampled = pd.concat([df_majority_downsampled, df_minority])\n\nprint(\"\\nClass Distribution after Undersampling:\")\nprint(df_undersampled['Class'].value_counts())\n\n# 2. SMOTE (Oversampling)\nX = df_fraud.drop('Class', axis=1)\ny = df_fraud['Class']\n\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X, y)\n\nprint(\"\\nClass Distribution after SMOTE:\")\nprint(pd.Series(y_smote).value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:28.384529Z","iopub.execute_input":"2026-02-27T17:50:28.384906Z","iopub.status.idle":"2026-02-27T17:50:29.033142Z","shell.execute_reply.started":"2026-02-27T17:50:28.384868Z","shell.execute_reply":"2026-02-27T17:50:29.032255Z"}},"outputs":[{"name":"stdout","text":"Original Class Distribution:\nClass\n0    284315\n1       492\nName: count, dtype: int64\n\nClass Distribution after Undersampling:\nClass\n0    492\n1    492\nName: count, dtype: int64\n\nClass Distribution after SMOTE:\nClass\n0    284315\n1    284315\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# *Task 10: Combining Multiple Datasets – MovieLens Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves integrating three separate data sources (Movies, Ratings, and Users) from the MovieLens dataset into a unified structure for analysis.\n\n**Technique:** Inner Merging (Joining) on common keys like movieId and userId.\n\n**Reason:** Merging allows us to relate user demographics (age/gender) to specific movie ratings and genres, which is essential for building recommendation systems or performing targeted data analysis.","metadata":{}},{"cell_type":"code","source":"path_movies = '/kaggle/input/datasets/organizations/grouplens/movielens-20m-dataset/movie.csv'\npath_ratings = '/kaggle/input/datasets/organizations/grouplens/movielens-20m-dataset/rating.csv'\n\n# 1. Load the individual datasets\ndf_movies = pd.read_csv(path_movies)\ndf_ratings = pd.read_csv(path_ratings)\n\n# 2. Merge Ratings with Movie Metadata\n# Common key is 'movieId'\ndf_combined = pd.merge(df_ratings, df_movies, on='movieId', how='inner')\n\n# Display Output\nprint(\"--- Movie Data Schema ---\")\nprint(df_movies.columns)\n\nprint(\"\\n--- Ratings Data Schema ---\")\nprint(df_ratings.columns)\n\nprint(\"\\n--- Combined Dataset (First 5 Rows) ---\")\nprint(df_combined[['userId', 'movieId', 'rating', 'title', 'genres']].head())\n\nprint(\"\\nShape of Combined Dataset:\", df_combined.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:29.034317Z","iopub.execute_input":"2026-02-27T17:50:29.034776Z","iopub.status.idle":"2026-02-27T17:50:56.813521Z","shell.execute_reply.started":"2026-02-27T17:50:29.034736Z","shell.execute_reply":"2026-02-27T17:50:56.812718Z"}},"outputs":[{"name":"stdout","text":"--- Movie Data Schema ---\nIndex(['movieId', 'title', 'genres'], dtype='object')\n\n--- Ratings Data Schema ---\nIndex(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')\n\n--- Combined Dataset (First 5 Rows) ---\n   userId  movieId  rating                                              title  \\\n0       1        2     3.5                                     Jumanji (1995)   \n1       1       29     3.5  City of Lost Children, The (Cité des enfants p...   \n2       1       32     3.5          Twelve Monkeys (a.k.a. 12 Monkeys) (1995)   \n3       1       47     3.5                        Seven (a.k.a. Se7en) (1995)   \n4       1       50     3.5                         Usual Suspects, The (1995)   \n\n                                   genres  \n0              Adventure|Children|Fantasy  \n1  Adventure|Drama|Fantasy|Mystery|Sci-Fi  \n2                 Mystery|Sci-Fi|Thriller  \n3                        Mystery|Thriller  \n4                  Crime|Mystery|Thriller  \n\nShape of Combined Dataset: (20000263, 6)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# *Task 11: Dimensionality Reduction – MNIST Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task focuses on reducing the dimensionality of the MNIST dataset (which has 784 pixel features per image) to a smaller set of features while retaining as much variance as possible.\n\n**Technique:** Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE).\n\n**Reason:** Reducing dimensions speeds up machine learning training times, mitigates the \"curse of dimensionality,\" and helps visualize high-dimensional data in 2D or 3D spaces.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Data loading - Replace with your copied path for train.csv\npath = '/kaggle/input/datasets/oddrationale/mnist-in-csv/mnist_train.csv' \ndf_mnist = pd.read_csv(path)\n\n# Separate features (pixels) and target (label)\nX = df_mnist.drop(columns=['label'])\ny = df_mnist['label']\n\n# 1. Principal Component Analysis (PCA)\n# Reduce to 50 components to retain high variance\npca = PCA(n_components=50)\nX_pca = pca.fit_transform(X)\n\nprint(\"--- PCA Results ---\")\nprint(f\"Original shape: {X.shape}\")\nprint(f\"Reduced shape: {X_pca.shape}\")\nprint(f\"Explained variance ratio (top 50 components): {np.sum(pca.explained_variance_ratio_):.4f}\")\n\n# 2. t-SNE (Basic Visualization)\n# Reduce to 2 components for visualization (subsetting data for speed)\ntsne = TSNE(n_components=2, random_state=42, n_iter=300)\nX_tsne = tsne.fit_transform(X[:2000]) # Using first 2000 images for faster output\n\nprint(\"\\n--- t-SNE Results ---\")\nprint(f\"Original shape: {X[:2000].shape}\")\nprint(f\"Reduced shape: {X_tsne.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:50:56.814770Z","iopub.execute_input":"2026-02-27T17:50:56.815172Z","iopub.status.idle":"2026-02-27T17:51:06.497933Z","shell.execute_reply.started":"2026-02-27T17:50:56.815143Z","shell.execute_reply":"2026-02-27T17:51:06.497003Z"}},"outputs":[{"name":"stdout","text":"--- PCA Results ---\nOriginal shape: (60000, 784)\nReduced shape: (60000, 50)\nExplained variance ratio (top 50 components): 0.8246\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n--- t-SNE Results ---\nOriginal shape: (2000, 784)\nReduced shape: (2000, 2)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# *Task 12: Text Preprocessing – IMDB Movie Reviews Dataset*\n","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves cleaning and formatting unstructured text data (movie reviews) so it can be converted into numerical vectors for machine learning models.\n\n**Technique:** Lowercasing, stopword removal, tokenization, and stemming.\n\n**Reason:** Unprocessed text contains noise (punctuation, capitalization) and redundant words (a, the, is) that do not add predictive value; cleaning reduces the feature space and improves model accuracy.","metadata":{}},{"cell_type":"code","source":"\n\n# Data loading \npath = '/kaggle/input/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows/imdb_top_1000.csv'\ndf_imdb = pd.read_csv(path)\n\ndf_imdb.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:51:06.499201Z","iopub.execute_input":"2026-02-27T17:51:06.500187Z","iopub.status.idle":"2026-02-27T17:51:06.533902Z","shell.execute_reply.started":"2026-02-27T17:51:06.500153Z","shell.execute_reply":"2026-02-27T17:51:06.532922Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                         Poster_Link  \\\n0  https://m.media-amazon.com/images/M/MV5BMDFkYT...   \n1  https://m.media-amazon.com/images/M/MV5BM2MyNj...   \n2  https://m.media-amazon.com/images/M/MV5BMTMxNT...   \n3  https://m.media-amazon.com/images/M/MV5BMWMwMG...   \n4  https://m.media-amazon.com/images/M/MV5BMWU4N2...   \n\n               Series_Title Released_Year Certificate  Runtime  \\\n0  The Shawshank Redemption          1994           A  142 min   \n1             The Godfather          1972           A  175 min   \n2           The Dark Knight          2008          UA  152 min   \n3    The Godfather: Part II          1974           A  202 min   \n4              12 Angry Men          1957           U   96 min   \n\n                  Genre  IMDB_Rating  \\\n0                 Drama          9.3   \n1          Crime, Drama          9.2   \n2  Action, Crime, Drama          9.0   \n3          Crime, Drama          9.0   \n4          Crime, Drama          9.0   \n\n                                            Overview  Meta_score  \\\n0  Two imprisoned men bond over a number of years...        80.0   \n1  An organized crime dynasty's aging patriarch t...       100.0   \n2  When the menace known as the Joker wreaks havo...        84.0   \n3  The early life and career of Vito Corleone in ...        90.0   \n4  A jury holdout attempts to prevent a miscarria...        96.0   \n\n               Director           Star1           Star2          Star3  \\\n0        Frank Darabont     Tim Robbins  Morgan Freeman     Bob Gunton   \n1  Francis Ford Coppola   Marlon Brando       Al Pacino     James Caan   \n2     Christopher Nolan  Christian Bale    Heath Ledger  Aaron Eckhart   \n3  Francis Ford Coppola       Al Pacino  Robert De Niro  Robert Duvall   \n4          Sidney Lumet     Henry Fonda     Lee J. Cobb  Martin Balsam   \n\n            Star4  No_of_Votes        Gross  \n0  William Sadler      2343110   28,341,469  \n1    Diane Keaton      1620367  134,966,411  \n2   Michael Caine      2303232  534,858,444  \n3    Diane Keaton      1129952   57,300,000  \n4    John Fiedler       689845    4,360,000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Poster_Link</th>\n      <th>Series_Title</th>\n      <th>Released_Year</th>\n      <th>Certificate</th>\n      <th>Runtime</th>\n      <th>Genre</th>\n      <th>IMDB_Rating</th>\n      <th>Overview</th>\n      <th>Meta_score</th>\n      <th>Director</th>\n      <th>Star1</th>\n      <th>Star2</th>\n      <th>Star3</th>\n      <th>Star4</th>\n      <th>No_of_Votes</th>\n      <th>Gross</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://m.media-amazon.com/images/M/MV5BMDFkYT...</td>\n      <td>The Shawshank Redemption</td>\n      <td>1994</td>\n      <td>A</td>\n      <td>142 min</td>\n      <td>Drama</td>\n      <td>9.3</td>\n      <td>Two imprisoned men bond over a number of years...</td>\n      <td>80.0</td>\n      <td>Frank Darabont</td>\n      <td>Tim Robbins</td>\n      <td>Morgan Freeman</td>\n      <td>Bob Gunton</td>\n      <td>William Sadler</td>\n      <td>2343110</td>\n      <td>28,341,469</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://m.media-amazon.com/images/M/MV5BM2MyNj...</td>\n      <td>The Godfather</td>\n      <td>1972</td>\n      <td>A</td>\n      <td>175 min</td>\n      <td>Crime, Drama</td>\n      <td>9.2</td>\n      <td>An organized crime dynasty's aging patriarch t...</td>\n      <td>100.0</td>\n      <td>Francis Ford Coppola</td>\n      <td>Marlon Brando</td>\n      <td>Al Pacino</td>\n      <td>James Caan</td>\n      <td>Diane Keaton</td>\n      <td>1620367</td>\n      <td>134,966,411</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://m.media-amazon.com/images/M/MV5BMTMxNT...</td>\n      <td>The Dark Knight</td>\n      <td>2008</td>\n      <td>UA</td>\n      <td>152 min</td>\n      <td>Action, Crime, Drama</td>\n      <td>9.0</td>\n      <td>When the menace known as the Joker wreaks havo...</td>\n      <td>84.0</td>\n      <td>Christopher Nolan</td>\n      <td>Christian Bale</td>\n      <td>Heath Ledger</td>\n      <td>Aaron Eckhart</td>\n      <td>Michael Caine</td>\n      <td>2303232</td>\n      <td>534,858,444</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://m.media-amazon.com/images/M/MV5BMWMwMG...</td>\n      <td>The Godfather: Part II</td>\n      <td>1974</td>\n      <td>A</td>\n      <td>202 min</td>\n      <td>Crime, Drama</td>\n      <td>9.0</td>\n      <td>The early life and career of Vito Corleone in ...</td>\n      <td>90.0</td>\n      <td>Francis Ford Coppola</td>\n      <td>Al Pacino</td>\n      <td>Robert De Niro</td>\n      <td>Robert Duvall</td>\n      <td>Diane Keaton</td>\n      <td>1129952</td>\n      <td>57,300,000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://m.media-amazon.com/images/M/MV5BMWU4N2...</td>\n      <td>12 Angry Men</td>\n      <td>1957</td>\n      <td>U</td>\n      <td>96 min</td>\n      <td>Crime, Drama</td>\n      <td>9.0</td>\n      <td>A jury holdout attempts to prevent a miscarria...</td>\n      <td>96.0</td>\n      <td>Sidney Lumet</td>\n      <td>Henry Fonda</td>\n      <td>Lee J. Cobb</td>\n      <td>Martin Balsam</td>\n      <td>John Fiedler</td>\n      <td>689845</td>\n      <td>4,360,000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\n\n\n# Download required NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Initialize tools\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:51:06.535980Z","iopub.execute_input":"2026-02-27T17:51:06.536406Z","iopub.status.idle":"2026-02-27T17:51:06.543795Z","shell.execute_reply.started":"2026-02-27T17:51:06.536364Z","shell.execute_reply":"2026-02-27T17:51:06.542677Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Text Preprocessing Function\n\ndef preprocess_text(text):\n    text = text.lower()                              # Lowercasing\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)          # Remove special characters\n    tokens = word_tokenize(text)                     # Tokenization\n    tokens = [word for word in tokens if word not in stop_words]  # Stopword removal\n    tokens = [stemmer.stem(word) for word in tokens] # Stemming\n    return \" \".join(tokens)\n\n# Apply Preprocessing\ndf_imdb['cleaned_review'] = df_imdb['Overview'].apply(preprocess_text)\n\n# Result Check\ndf_imdb[['Overview', 'cleaned_review']].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:51:06.544995Z","iopub.execute_input":"2026-02-27T17:51:06.545262Z","iopub.status.idle":"2026-02-27T17:51:06.927749Z","shell.execute_reply.started":"2026-02-27T17:51:06.545237Z","shell.execute_reply":"2026-02-27T17:51:06.927026Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"                                            Overview  \\\n0  Two imprisoned men bond over a number of years...   \n1  An organized crime dynasty's aging patriarch t...   \n2  When the menace known as the Joker wreaks havo...   \n3  The early life and career of Vito Corleone in ...   \n4  A jury holdout attempts to prevent a miscarria...   \n\n                                      cleaned_review  \n0  two imprison men bond number year find solac e...  \n1  organ crime dynasti age patriarch transfer con...  \n2  menac known joker wreak havoc chao peopl gotha...  \n3  earli life career vito corleon new york citi p...  \n4  juri holdout attempt prevent miscarriag justic...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Overview</th>\n      <th>cleaned_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Two imprisoned men bond over a number of years...</td>\n      <td>two imprison men bond number year find solac e...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>An organized crime dynasty's aging patriarch t...</td>\n      <td>organ crime dynasti age patriarch transfer con...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>When the menace known as the Joker wreaks havo...</td>\n      <td>menac known joker wreak havoc chao peopl gotha...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The early life and career of Vito Corleone in ...</td>\n      <td>earli life career vito corleon new york citi p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A jury holdout attempts to prevent a miscarria...</td>\n      <td>juri holdout attempt prevent miscarriag justic...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"# *Task 13: Time-Series Preprocessing – Air Quality Dataset*","metadata":{}},{"cell_type":"markdown","source":"**Introduction:** This task involves cleaning and preparing time-series data, which has a specific temporal ordering, for analysis.\n\n**Technique:** Handling missing timestamps (index setting), Resampling, and Smoothing (Rolling Mean).\n\n**Reason:** Time-series data often has irregular intervals or gaps; resampling forces a regular frequency (e.g., daily), while smoothing reduces noise to highlight underlying trends.","metadata":{}},{"cell_type":"code","source":"\npath = '/kaggle/input/datasets/fedesoriano/air-quality-data-set/AirQuality.csv'\ndf_air = pd.read_csv(path, sep=';', decimal=',')\n\n# Convert Date and Time columns to a single Datetime index\ndf_air['DateTime'] = pd.to_datetime(df_air['Date'] + ' ' + df_air['Time'], format='%d/%m/%Y %H.%M.%S', errors='coerce')\ndf_air.set_index('DateTime', inplace=True)\n\n# Dropping rows with completely missing datetimes\ndf_air.dropna(subset=['CO(GT)'], inplace=True)\n\nprint(\"Original Time Series (First 5 Rows):\")\nprint(df_air[['CO(GT)']].head())\n\n# 1. Resampling (Downsample to Daily Mean)\ndf_daily = df_air['CO(GT)'].resample('D').mean()\nprint(\"\\n--- Daily Resampled Data (First 5 Rows) ---\")\nprint(df_daily.head())\n\n# 2. Smoothing (Rolling Mean - 7 Day Window)\ndf_smoothed = df_daily.rolling(window=7, min_periods=1).mean()\nprint(\"\\n--- 7-Day Rolling Mean (First 5 Rows) ---\")\nprint(df_smoothed.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-27T17:51:06.928843Z","iopub.execute_input":"2026-02-27T17:51:06.929190Z","iopub.status.idle":"2026-02-27T17:51:07.022629Z","shell.execute_reply.started":"2026-02-27T17:51:06.929154Z","shell.execute_reply":"2026-02-27T17:51:07.021629Z"}},"outputs":[{"name":"stdout","text":"Original Time Series (First 5 Rows):\n                     CO(GT)\nDateTime                   \n2004-03-10 18:00:00     2.6\n2004-03-10 19:00:00     2.0\n2004-03-10 20:00:00     2.2\n2004-03-10 21:00:00     2.2\n2004-03-10 22:00:00     1.6\n\n--- Daily Resampled Data (First 5 Rows) ---\nDateTime\n2004-03-10     1.966667\n2004-03-11    -6.187500\n2004-03-12   -14.095833\n2004-03-13    -5.750000\n2004-03-14    -5.966667\nFreq: D, Name: CO(GT), dtype: float64\n\n--- 7-Day Rolling Mean (First 5 Rows) ---\nDateTime\n2004-03-10    1.966667\n2004-03-11   -2.110417\n2004-03-12   -6.105556\n2004-03-13   -6.016667\n2004-03-14   -6.006667\nFreq: D, Name: CO(GT), dtype: float64\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}